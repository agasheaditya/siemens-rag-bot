{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bade20f-93f6-415e-b5f1-5eb0b5a8a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e04f33b-2ad3-4030-a98f-b9618d321a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a16d77b8-4d9e-44cc-bc86-c72cae0f7927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c12s05.pdf', 'The Making of Iron & Steel.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../data/\"\n",
    "file_names = os.listdir(data_path)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13693d9f-dc64-442e-9634-3c8850618c7c",
   "metadata": {},
   "source": [
    "## Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a240c00c-725f-45de-ae0f-5ee2cdeb3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def semantic_chunking(text, similarity_threshold=0.75):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks, current_chunk = [], [sentences[0]]\n",
    "    \n",
    "    for i in range(1, len(sentences)):\n",
    "        similarity = util.cos_sim(\n",
    "            model.encode(current_chunk[-1], convert_to_tensor=True),\n",
    "            model.encode(sentences[i], convert_to_tensor=True)\n",
    "        ).item()\n",
    "        \n",
    "        if similarity < similarity_threshold:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentences[i]]\n",
    "        else:\n",
    "            current_chunk.append(sentences[i])\n",
    "    \n",
    "    chunks.append(\" \".join(current_chunk))  # Add the final chunk\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf269a4-06d9-437a-b796-51f1b0839341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticTextSplitter(TextSplitter):\n",
    "    def __init__(self, similarity_threshold=0.75):\n",
    "        super().__init__()\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    def split_text(self, text):\n",
    "        return semantic_chunking(text, self.similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39824175-1939-4dd1-a7f6-f7b4afb645e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for file in file_names:\n",
    "    full_file_name = os.path.join(data_path, file)\n",
    "    loader = PyPDFLoader(full_file_name)\n",
    "    # Use the custom splitter\n",
    "    semantic_splitter = SemanticTextSplitter(similarity_threshold=0.75)\n",
    "    documents += loader.load_and_split(text_splitter=semantic_splitter)\n",
    "\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196eb070-1ce1-448b-b097-8b0a57929da4",
   "metadata": {},
   "source": [
    "## Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adca2ea3-79da-4bdb-a900-8815e0ccfbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_24980\\3184004565.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# Generate embeddings for your documents\n",
    "embeddings = [embedding_model.embed_query(doc.page_content) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d79b9518-a3af-4ac2-bcd0-37c64b16df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Chroma DB instance and store embeddings\n",
    "chroma_db = Chroma.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b6011af-c6c1-4541-a91b-e7bad72fe1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5.1.2 Iron Production -\n",
      "Iron is produced in blast furnaces by the reduction of iron bearing materials with a hot gas.\n",
      "\n",
      "\n",
      "The iron is also used for feed in blast furnaces and BOF's when economics allow.\n",
      "\n",
      "\n",
      "Production \n",
      "of iron in the blast furnace is a thermo chemical process, during which the metal is reduced from \n",
      "its oxides by a series of chemical reactions and carburised to reduce its melting temperature.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "query = \"which furnace is used to produce the iron\"\n",
    "results = chroma_db.similarity_search(query, k=3)  # Retrieve top 3 relevant chunks\n",
    "for result in results:\n",
    "    print(result.page_content.rstrip())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75946c8d-6d2e-472b-a780-796766c36219",
   "metadata": {},
   "source": [
    "## Step : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3141ffa3-268f-404c-ac3c-04e0d680826d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0167f0048ce04a6eab486ea8439a50ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # Replace with your chosen model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91200dc6-ec8c-424d-b65a-cc49ac4b45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test with a sample query\n",
    "# def generate_response(prompt):\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "#     outputs = model.generate(inputs.input_ids, max_length=200, temperature=0.1)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return response\n",
    "\n",
    "# prompt =  \"which furnace is used to produce the iron\"\n",
    "# response = generate_response(prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b5c022-c491-4b04-be47-dc456a96cbde",
   "metadata": {},
   "source": [
    "## Step 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeee4333-a818-4560-ab94-e6a6377351fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2982dd82cc58483198ad541b410fdfa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "# Choose a model\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # Replace with your choice\n",
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=200.0)\n",
    "# Load tokenizer and quantized model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "925161c7-6786-44cf-be16-2ffc57766759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pre-trained embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Store embeddings in ChromaDB\n",
    "vectorstore = Chroma.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd6f0cf0-7d2e-42f5-b185-e0a2625632db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query):\n",
    "    return vectorstore.similarity_search(query, k=3)  # Retrieve top 5 chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73b23127-7b23-49a9-8982-cc80d080cfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the following context:\n",
      "12.5.1.2 Iron Production -\n",
      "Iron is produced in blast furnaces by the reduction of iron bearing materials with a hot gas.\n",
      "\n",
      "12.5.1.2 Iron Production -\n",
      "Iron is produced in blast furnaces by the reduction of iron bearing materials with a hot gas.\n",
      "\n",
      "The iron is also used for feed in blast furnaces and BOF's when economics allow.\n",
      "\n",
      "Question: which furnace is used to produce the iron?\n",
      "Answer: Blast Furnace.\n",
      "\n",
      "Explanation: The process of producing iron in a blast furnace involves the reduction of iron bearing materials with a hot gas, resulting in the production of pig iron. The pig iron is then used as feed in blast furnaces and Basic Oxygen Furnaces (BOFs) when economics allow. Therefore, the answer is\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(query):\n",
    "    # Retrieve relevant chunks\n",
    "    docs = retrieve_relevant_chunks(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Combine context with user query\n",
    "    prompt = f\"Answer the question based on the following context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=200, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example query\n",
    "query = \"which furnace is used to produce the iron?\"\n",
    "answer = generate_answer(query)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea08338-116f-4d68-b930-3a817f852930",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b16c9d4-c7db-4fea-8ffd-7273aab65413",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = {\n",
    "    \"which furnace is used to produce the iron?\": [\"Iron is produced in blast furnaces by the reduction of iron bearing materials with a hot gas.\",\n",
    "                                                  \"The iron is also used for feed in blast furnaces and BOF's when economics allow.\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "148ca8ac-19e0-47fc-a869-17036e586f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@3: 0.3333333333333333\n",
      "Recall@3: 0.5\n"
     ]
    }
   ],
   "source": [
    "def precision_at_k(retrieved_docs, relevant_docs, k):\n",
    "    retrieved_k = retrieved_docs[:k]\n",
    "    relevant_and_retrieved = set(retrieved_k) & set(relevant_docs)\n",
    "    return len(relevant_and_retrieved) / k\n",
    "\n",
    "def recall_at_k(retrieved_docs, relevant_docs, k):\n",
    "    retrieved_k = retrieved_docs[:k]\n",
    "    relevant_and_retrieved = set(retrieved_k) & set(relevant_docs)\n",
    "    return len(relevant_and_retrieved) / len(relevant_docs)\n",
    "\n",
    "# Example\n",
    "retrieved_docs = [\"12.5.1.2 Iron Production - Iron is produced in blast furnaces by the reduction of iron bearing materials with a hot gas.\", \n",
    "                  \"12.5.1.2 Iron Production -Iron is produced in blast furnaces by the reduction of iron bearing materials with a hot gas.\", \"The iron is also used for feed in blast furnaces and BOF's when economics allow.\"]\n",
    "relevant_docs = ground_truth[\"which furnace is used to produce the iron?\"]\n",
    "k = 3\n",
    "\n",
    "precision = precision_at_k(retrieved_docs, relevant_docs, k)\n",
    "recall = recall_at_k(retrieved_docs, relevant_docs, k)\n",
    "print(f\"Precision@{k}: {precision}\")\n",
    "print(f\"Recall@{k}: {recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95165f4c-9a03-4c70-bee1-f9b3e9aec16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@3: 0.3065735963827292\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ndcg_at_k(retrieved_docs, relevant_docs, k):\n",
    "    retrieved_k = retrieved_docs[:k]\n",
    "    dcg = sum([1 / np.log2(idx + 2) if doc in relevant_docs else 0 for idx, doc in enumerate(retrieved_k)])\n",
    "    ideal_dcg = sum([1 / np.log2(idx + 2) for idx in range(min(len(relevant_docs), k))])\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "# Example\n",
    "ndcg = ndcg_at_k(retrieved_docs, relevant_docs, k)\n",
    "print(f\"nDCG@{k}: {ndcg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1cd21-3bf7-444a-8704-677496377c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=0.11764705882352941, fmeasure=0.21052631578947367), 'rouge2': Score(precision=1.0, recall=0.0625, fmeasure=0.11764705882352941), 'rougeL': Score(precision=1.0, recall=0.11764705882352941, fmeasure=0.21052631578947367)}\n",
      "BLEU Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu.metrics import BLEU\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# Ground truth and generated response\n",
    "reference = \"Iron is produced in blast furnaces by the reduction of iron bearing materials with a hot gas.\"\n",
    "generated = \"Blast Furnace.\"\n",
    "\n",
    "# ROUGE\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = rouge.score(reference, generated)\n",
    "print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "\n",
    "# BLEU\n",
    "bleu = BLEU()\n",
    "bleu_score = bleu.corpus_score([generated], [[reference]])\n",
    "print(f\"BLEU Score: {bleu_score.score}\")\n",
    "\n",
    "# BERTScore\n",
    "P, R, F1 = bert_score([generated], [reference], lang=\"en\")\n",
    "print(f\"BERTScore: P={P.mean().item()}, R={R.mean().item()}, F1={F1.mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e6193-ec3c-4cec-bf29-fc7859c530fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d10de-5458-4c01-ade3-64f9edb977dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06827de369b2449caa0314324952738f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0768f2b650c740a1b24c87ac2896293a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2982dd82cc58483198ad541b410fdfa9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_539eceb9b83b47608bdf0ead611b7bd7",
        "IPY_MODEL_c0742425f0e74439b0a7355730b363a9",
        "IPY_MODEL_7a91baf01c82418fa9340a3bae3d6ba4"
       ],
       "layout": "IPY_MODEL_77cb0c620b0f44eca0ca38484627a2e0"
      }
     },
     "49bf585707924abb81b0ef7e53e25cb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "539eceb9b83b47608bdf0ead611b7bd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5876accbb71342759332af785e40fce4",
       "style": "IPY_MODEL_06827de369b2449caa0314324952738f",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "53fd82a827ef4c3ab00880429380aa57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5876accbb71342759332af785e40fce4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "77cb0c620b0f44eca0ca38484627a2e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7a91baf01c82418fa9340a3bae3d6ba4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7ee7838706dd4748bc156b2fe67559a9",
       "style": "IPY_MODEL_0768f2b650c740a1b24c87ac2896293a",
       "value": " 2/2 [00:07&lt;00:00,  7.22s/it]"
      }
     },
     "7ee7838706dd4748bc156b2fe67559a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c0742425f0e74439b0a7355730b363a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_49bf585707924abb81b0ef7e53e25cb7",
       "max": 2,
       "style": "IPY_MODEL_53fd82a827ef4c3ab00880429380aa57",
       "value": 2
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
