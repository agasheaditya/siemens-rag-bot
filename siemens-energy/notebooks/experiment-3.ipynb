{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22740d32-73c2-4682-b048-b5e3775f4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853e27a4-5c6f-43bd-a3e3-7c92fe6d7293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\diffusers\\pipelines\\pipeline_loading_utils.py:285: FutureWarning: You are loading the variant fp16 from CompVis/stable-diffusion-v1-4 via `revision='fp16'`. This behavior is deprecated and will be removed in diffusers v1. One should use `variant='fp16'` instead. However, it appears that CompVis/stable-diffusion-v1-4 currently does not have the required variant filenames in the 'main' branch. \n",
      " The Diffusers team and community would be very grateful if you could open an issue: https://github.com/huggingface/diffusers/issues/new with the title 'CompVis/stable-diffusion-v1-4 is missing fp16 files' so that the correct variant file can be added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a266149b44cb4d4c8752573d9a0459e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "D:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "An error occurred while trying to fetch C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    revision=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Freeze VAE and Text Encoder\n",
    "pipe.vae.requires_grad_(False)\n",
    "pipe.text_encoder.requires_grad_(False)\n",
    "\n",
    "unet = pipe.unet\n",
    "tokenizer = pipe.tokenizer\n",
    "text_encoder = pipe.text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed71dbee-abe1-4cac-8447-572f83bdf6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA injected successfully into UNet!\n"
     ]
    }
   ],
   "source": [
    "# Set LoRA Adapters for UNet\n",
    "rank = 4  # LoRA rank\n",
    "\n",
    "# Iterate through all attention processors\n",
    "for name, module in unet.attn_processors.items():\n",
    "    if isinstance(module, LoRAAttnProcessor):\n",
    "        continue  # Already LoRA\n",
    "    cross_attention_dim = module.cross_attention_dim if hasattr(module, \"cross_attention_dim\") else None\n",
    "    hidden_size = module.hidden_size if hasattr(module, \"hidden_size\") else None\n",
    "\n",
    "    if cross_attention_dim is None or hidden_size is None:\n",
    "        continue\n",
    "\n",
    "    # Create LoRA processor\n",
    "    lora_attn_processor = LoRAAttnProcessor(\n",
    "        hidden_size=hidden_size,\n",
    "        cross_attention_dim=cross_attention_dim,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "    # Set it\n",
    "    unet.set_attn_processor(name, lora_attn_processor)\n",
    "\n",
    "print(\"LoRA injected successfully into UNet!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "391be507-1a65-40f4-9ee9-9bd317abbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleDefectDataset(Dataset):\n",
    "    def __init__(self, image_paths, captions, tokenizer, resolution=(512,512)):\n",
    "        self.image_paths = image_paths\n",
    "        self.captions = captions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        image = image.resize(self.resolution)\n",
    "        image = np.array(image).astype(np.float32) / 255.0\n",
    "        image = torch.tensor(image).permute(2,0,1)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            self.captions[idx],\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"input_ids\": inputs.input_ids.squeeze(0),\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a910ca-3570-4144-ad66-5492b13c62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1, path_2 = \"../dataset/bottle/image/broken_large-000.png\", \"../dataset/bottle/image/broken_large-001.png\"\n",
    "caption_1, caption_2 = \"../dataset/bottle/image/broken_large-000.txt\", \"../dataset/bottle/image/broken_large-001.txt\"\n",
    "train_dataset = BottleDefectDataset(\n",
    "    image_paths=[path_1, path_2],  # your 2 images per defect\n",
    "    captions=[caption_1, caption_2], \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deaf18f1-082e-4e6a-be3e-6a8d06da5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, unet.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fcedaff-102c-4144-b130-773461b0a85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: nan: 100%|██████████| 2/2 [00:43<00:00, 21.65s/it]   \n",
      "Loss: nan: 100%|██████████| 2/2 [00:36<00:00, 18.38s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.76s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.53s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:36<00:00, 18.41s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.56s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.54s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.85s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.58s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.58s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.58s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.99s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:36<00:00, 18.18s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.99s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.89s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.55s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.63s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.83s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.64s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.58s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.71s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:38<00:00, 19.02s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.55s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.70s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 19.00s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.84s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.56s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.62s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.68s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.57s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.56s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.66s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.60s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.64s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.69s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.58s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.83s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.56s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.73s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.76s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.71s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.78s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.54s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.74s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.62s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.52s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.69s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.64s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.58s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.65s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "unet.train()\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        latents = pipe.vae.encode(batch[\"pixel_values\"].to(\"cuda\").half()).latent_dist.sample()\n",
    "        latents = latents * 0.18215  # VAE scaling\n",
    "\n",
    "        encoder_hidden_states = pipe.text_encoder(batch[\"input_ids\"].to(\"cuda\"))[0]\n",
    "\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=latents.device).long()\n",
    "\n",
    "        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0802052-10c8-4f30-8efc-d735438ed0d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UNet2DConditionModel' object has no attribute 'peft_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43munet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_attn_procs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaved_lora_adapters/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\diffusers\\loaders\\unet.py:491\u001b[0m, in \u001b[0;36mUNet2DConditionLoadersMixin.save_attn_procs\u001b[1;34m(self, save_directory, is_main_process, weight_name, save_function, safe_serialization, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPEFT backend is required for saving LoRAs using the `save_attn_procs()` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_peft_model_state_dict\n\u001b[1;32m--> 491\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n",
      "File \u001b[1;32mD:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:75\u001b[0m, in \u001b[0;36mget_peft_model_state_dict\u001b[1;34m(model, state_dict, adapter_name, unwrap_compiled, save_embedding_layers)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unwrap_compiled:\n\u001b[0;32m     73\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_orig_mod\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[1;32m---> 75\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_config\u001b[49m[adapter_name]\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "File \u001b[1;32mD:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\diffusers\\models\\modeling_utils.py:291\u001b[0m, in \u001b[0;36mModelMixin.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_dict[name]\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# call PyTorch's https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'UNet2DConditionModel' object has no attribute 'peft_config'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540c273-03df-483a-a393-48486c3f4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet.load_attn_procs(\"saved_lora_adapters/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "061adc353bd34e9b849d5ab4bcca4c31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "23c70ed780fc4be9b788c515cf586aa9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3e05a889400d4d73bfc660aad4781b29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4af585d76b64400c85c4a5c50cd4528c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_061adc353bd34e9b849d5ab4bcca4c31",
       "style": "IPY_MODEL_f395ca95a3074555be399205baf3e842",
       "value": "Loading pipeline components...: 100%"
      }
     },
     "56046079d9aa42e6a3cf87aa64e1e43e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a266149b44cb4d4c8752573d9a0459e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4af585d76b64400c85c4a5c50cd4528c",
        "IPY_MODEL_a5ed32fcc64249c6971d7fdc795ba300",
        "IPY_MODEL_ddcaabe6d68d4e558d6fe4718c22faf4"
       ],
       "layout": "IPY_MODEL_3e05a889400d4d73bfc660aad4781b29"
      }
     },
     "a5ed32fcc64249c6971d7fdc795ba300": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_b97e459f37054a6283e73b5c862b6450",
       "max": 7,
       "style": "IPY_MODEL_a7203b30584a453188d8a0d44211a672",
       "value": 7
      }
     },
     "a7203b30584a453188d8a0d44211a672": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b97e459f37054a6283e73b5c862b6450": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ddcaabe6d68d4e558d6fe4718c22faf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_56046079d9aa42e6a3cf87aa64e1e43e",
       "style": "IPY_MODEL_23c70ed780fc4be9b788c515cf586aa9",
       "value": " 7/7 [00:00&lt;00:00, 10.02it/s]"
      }
     },
     "f395ca95a3074555be399205baf3e842": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
