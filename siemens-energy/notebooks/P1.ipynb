{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22f8d4bd-f171-4914-9edb-61fbf220cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers.schedulers import DDPMScheduler\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "import torch.optim as optim\n",
    "from diffusers.schedulers import DDPMScheduler\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "# Tokenize and encode captions\n",
    "tokenizer = pipe.tokenizer\n",
    "text_encoder = pipe.text_encoder.to(\"cuda\")  # Ensure text encoder is on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f6dc8b-f6c2-47dd-b77e-fea8a192036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, caption_file_dir, image_transform=None, mask_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.caption_file_dir = caption_file_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_files[idx]\n",
    "        mask_name = image_name.split(\".\")  # Assuming mask has the same name\n",
    "        mask_name = mask_name[0] + \"_rbg_mask.\" + mask_name[1]\n",
    "        caption_file_name = os.path.splitext(image_name)[0] + '.txt'\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "        caption_path = os.path.join(self.caption_file_dir, caption_file_name)\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Single-channel mask\n",
    "        with open(caption_path, \"r\") as f:\n",
    "            caption = f.read().strip()\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb28a47-03ce-4f3a-9d2f-d8d4ac4b8fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your data directories\n",
    "image_directory = '../dataset/bottle/image/'\n",
    "mask_directory = '../dataset/bottle/rbg_mask/'\n",
    "caption_directory = '../dataset/bottle/captions/'\n",
    "\n",
    "# Define image transformations (resize, normalize)\n",
    "# Define transformations for images\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize for RGB images\n",
    "    transforms.Lambda(lambda x: x.half())  # Cast to torch.float16\n",
    "])\n",
    "\n",
    "# Define transformations for masks\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),  # Convert mask to tensor without normalization\n",
    "    transforms.Lambda(lambda x: x.half())  # Cast to torch.float16\n",
    "\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = DefectDataset(image_directory, mask_directory, caption_directory, image_transform=image_transform, mask_transform=mask_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45633ccb-52df-4849-a921-2b1f43b52d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 256, 256]), Mask shape: torch.Size([1, 256, 256]), Caption: The image shows a top-down view of a bottle, looking from the bottle's neck towards the bottom, with a white background. The bottom right corner has a large breakage defect, shaped like a quarter of a ring, in black color.\n"
     ]
    }
   ],
   "source": [
    "# Example of accessing an item\n",
    "image, mask, caption = dataset[0]\n",
    "print(f\"Image shape: {image.shape}, Mask shape: {mask.shape}, Caption: {caption}\")\n",
    "\n",
    "# You'll then need to create DataLoaders for training and validation\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a832a8-2fa6-47d0-abf6-20ec74f0bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(image_directory, mask_directory, caption_directory, batch_size=2):\n",
    "\n",
    "    dataset = DefectDataset(image_directory, mask_directory, caption_directory, image_transform=image_transform, mask_transform=mask_transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989a94c2-4b16-4a00-8713-872ce2aaee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "dataloader = get_dataloader(image_directory, mask_directory, caption_directory)\n",
    "dataloader = accelerator.prepare(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f58f5c9-0845-4d65-9820-46900afa8445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2417cc01a50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0587b6-b4f0-4dc7-9059-2ede50379228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9899ccb4d7574b5ca9af2eaa16714000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.33.1\",\n",
       "  \"_name_or_path\": \"runwayml/stable-diffusion-v1-5\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    \"stable_diffusion\",\n",
       "    \"StableDiffusionSafetyChecker\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"DPMSolverMultistepScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe.text_encoder.to(\"cpu\")  # Move text encoder to CPU\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57af2c73-8319-4262-ac7c-e9af2f117de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "unwrapped_model = pipe.unet\n",
    "lora_model = get_peft_model(unwrapped_model, lora_config)\n",
    "\n",
    "# Optimizer and GradScaler\n",
    "optimizer = optim.AdamW(lora_model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b72d8da-71b0-4468-b52f-a19c78babb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd37aefe-c687-44e1-8438-d552f06119db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 32, 32]), torch.Size([2]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_latents.shape, timesteps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce836527-1ad6-4faf-b7c6-a8ff156c2c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"The image shows a top-down view of a bottle, looking from the bottle's neck towards the bottom, with a white background. The lower left corner has a small breakage defect, shaped like a small part of a ring, in black color.\",\n",
       " \"The image shows a top-down view of a bottle, looking from the bottle's neck towards the bottom, with a white background. The lower right corner has a large breakage defect, shaped like a small part of a ring, in black color.\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98394a03-9b48-4b16-b89b-5edd4b5878f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_captions(captions, max_length=77):\n",
    "    inputs = tokenizer(\n",
    "        captions,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "    \n",
    "    # Encode captions into embeddings\n",
    "    encoder_hidden_states = text_encoder(input_ids, attention_mask)[0]\n",
    "    return encoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115172fc-7612-4843-8569-ca495c98d266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.6318359375\n",
      "Epoch 0, Loss: 1.05859375\n",
      "Epoch 0, Loss: 0.94775390625\n",
      "Epoch 0, Loss: 1.00390625\n",
      "Epoch 0, Loss: 0.468994140625\n",
      "Epoch 0, Loss: 0.73486328125\n",
      "Epoch 0, Loss: 0.4365234375\n"
     ]
    }
   ],
   "source": [
    "accumulation_steps = 1  # Simulate a batch size of 4 * 4 = 16\n",
    "for epoch in range(5):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Extract batch data\n",
    "        images, masks, captions = batch #batch[\"image\"], batch[\"mask\"], batch[\"caption\"]\n",
    "        \n",
    "        # Ensure inputs are in torch.float16 and moved to GPU\n",
    "        images = images.half().to(\"cuda\")\n",
    "        masks = masks.half().to(\"cuda\")\n",
    "        \n",
    "        # Encode images into latent space\n",
    "        latents = pipe.vae.encode(images).latent_dist.sample()\n",
    "        \n",
    "        # Generate noise and noisy latents\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, pipe.scheduler.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n",
    "        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        # Preprocess captions\n",
    "        encoder_hidden_states = preprocess_captions(captions)\n",
    "        \n",
    "        # Predict noise\n",
    "        output = lora_model(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states)\n",
    "        noise_pred = output[0]  # Adjust based on output structure\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4922c2ce-5c29-4145-bde3-4bc58a904c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "23e13a1e57f843ee817d4bd8c40e4d53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4f6371477bfd4536b05036460980c0ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "51a8dbcef0cc4bc88d3ce8d56903f50a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "706867a0a952416fb68270799e6851fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "811b348634554f51933cb19c6eba91b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "893ea43dedc34e60b04e22d8730c68ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_811b348634554f51933cb19c6eba91b3",
       "max": 7,
       "style": "IPY_MODEL_23e13a1e57f843ee817d4bd8c40e4d53",
       "value": 7
      }
     },
     "9899ccb4d7574b5ca9af2eaa16714000": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d4dda693ef7842f7a87e2ffbbc9986db",
        "IPY_MODEL_893ea43dedc34e60b04e22d8730c68ca",
        "IPY_MODEL_eb1c68a8a88841acbba87a7452bb8912"
       ],
       "layout": "IPY_MODEL_4f6371477bfd4536b05036460980c0ec"
      }
     },
     "c147f1d9b0d64a76bb26f5e896cce41d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d4dda693ef7842f7a87e2ffbbc9986db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c147f1d9b0d64a76bb26f5e896cce41d",
       "style": "IPY_MODEL_51a8dbcef0cc4bc88d3ce8d56903f50a",
       "value": "Loading pipeline components...: 100%"
      }
     },
     "e43e97b6ef304624b212f9f7ff97e661": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "eb1c68a8a88841acbba87a7452bb8912": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e43e97b6ef304624b212f9f7ff97e661",
       "style": "IPY_MODEL_706867a0a952416fb68270799e6851fa",
       "value": " 7/7 [00:09&lt;00:00,  1.50s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
