{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22740d32-73c2-4682-b048-b5e3775f4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853e27a4-5c6f-43bd-a3e3-7c92fe6d7293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\diffusers\\pipelines\\pipeline_loading_utils.py:285: FutureWarning: You are loading the variant fp16 from CompVis/stable-diffusion-v1-4 via `revision='fp16'`. This behavior is deprecated and will be removed in diffusers v1. One should use `variant='fp16'` instead. However, it appears that CompVis/stable-diffusion-v1-4 currently does not have the required variant filenames in the 'main' branch. \n",
      " The Diffusers team and community would be very grateful if you could open an issue: https://github.com/huggingface/diffusers/issues/new with the title 'CompVis/stable-diffusion-v1-4 is missing fp16 files' so that the correct variant file can be added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a266149b44cb4d4c8752573d9a0459e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "D:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "An error occurred while trying to fetch C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    revision=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Freeze VAE and Text Encoder\n",
    "pipe.vae.requires_grad_(False)\n",
    "pipe.text_encoder.requires_grad_(False)\n",
    "\n",
    "unet = pipe.unet\n",
    "tokenizer = pipe.tokenizer\n",
    "text_encoder = pipe.text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed71dbee-abe1-4cac-8447-572f83bdf6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA injected successfully into UNet!\n"
     ]
    }
   ],
   "source": [
    "# Set LoRA Adapters for UNet\n",
    "rank = 4  # LoRA rank\n",
    "\n",
    "# Iterate through all attention processors\n",
    "for name, module in unet.attn_processors.items():\n",
    "    if isinstance(module, LoRAAttnProcessor):\n",
    "        continue  # Already LoRA\n",
    "    cross_attention_dim = module.cross_attention_dim if hasattr(module, \"cross_attention_dim\") else None\n",
    "    hidden_size = module.hidden_size if hasattr(module, \"hidden_size\") else None\n",
    "\n",
    "    if cross_attention_dim is None or hidden_size is None:\n",
    "        continue\n",
    "\n",
    "    # Create LoRA processor\n",
    "    lora_attn_processor = LoRAAttnProcessor(\n",
    "        hidden_size=hidden_size,\n",
    "        cross_attention_dim=cross_attention_dim,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "    # Set it\n",
    "    unet.set_attn_processor(name, lora_attn_processor)\n",
    "\n",
    "print(\"LoRA injected successfully into UNet!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "391be507-1a65-40f4-9ee9-9bd317abbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleDefectDataset(Dataset):\n",
    "    def __init__(self, image_paths, captions, tokenizer, resolution=(512,512)):\n",
    "        self.image_paths = image_paths\n",
    "        self.captions = captions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        image = image.resize(self.resolution)\n",
    "        image = np.array(image).astype(np.float32) / 255.0\n",
    "        image = torch.tensor(image).permute(2,0,1)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            self.captions[idx],\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"input_ids\": inputs.input_ids.squeeze(0),\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a910ca-3570-4144-ad66-5492b13c62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1, path_2 = \"../dataset/bottle/image/broken_large-000.png\", \"../dataset/bottle/image/broken_large-001.png\"\n",
    "caption_1, caption_2 = \"../dataset/bottle/image/broken_large-000.txt\", \"../dataset/bottle/image/broken_large-001.txt\"\n",
    "train_dataset = BottleDefectDataset(\n",
    "    image_paths=[path_1, path_2],  # your 2 images per defect\n",
    "    captions=[caption_1, caption_2], \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deaf18f1-082e-4e6a-be3e-6a8d06da5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, unet.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fcedaff-102c-4144-b130-773461b0a85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: nan: 100%|██████████| 2/2 [00:43<00:00, 21.65s/it]   \n",
      "Loss: nan: 100%|██████████| 2/2 [00:36<00:00, 18.38s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.76s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.53s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:36<00:00, 18.41s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.56s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.54s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.85s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.58s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.58s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.58s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.99s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:36<00:00, 18.18s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.99s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.89s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.55s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.63s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.83s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.64s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.58s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.71s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:38<00:00, 19.02s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.55s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.70s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 19.00s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.84s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.56s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.62s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.68s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.57s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.56s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.66s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.60s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.64s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.69s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.58s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.83s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.56s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.73s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.76s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.71s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.78s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.54s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.74s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.62s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.52s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.69s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.64s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.58s/it]\n",
      "Loss: nan: 100%|██████████| 2/2 [00:37<00:00, 18.65s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "unet.train()\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        latents = pipe.vae.encode(batch[\"pixel_values\"].to(\"cuda\").half()).latent_dist.sample()\n",
    "        latents = latents * 0.18215  # VAE scaling\n",
    "\n",
    "        encoder_hidden_states = pipe.text_encoder(batch[\"input_ids\"].to(\"cuda\"))[0]\n",
    "\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=latents.device).long()\n",
    "\n",
    "        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0802052-10c8-4f30-8efc-d735438ed0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet.save_lora_adapter(\"saved_lora_adapters/\", adapter_name=\"damage_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540c273-03df-483a-a393-48486c3f4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.unet.load_attn_procs(\"saved_lora_adapters/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b6d3f7a-f40b-419e-9705-45415474fff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e58a470a04f4f4ab9313a1ab06b67bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now prompt new synthetic images\n",
    "\n",
    "prompt = \"A damaged bottle with a crack at the bottom\"\n",
    "negative_prompt = \"blurry, low quality, distorted\"\n",
    "\n",
    "# Generate synthetic defect image\n",
    "# pipe.safety_checker = None\n",
    "pipe.safety_checker = None # lambda images, clip_input: (images, False)\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    # negative_prompt=negative_prompt,\n",
    "    num_inference_steps=5,\n",
    "    guidance_scale=1,\n",
    ").images[0]\n",
    "\n",
    "# Show the image\n",
    "image.show()\n",
    "\n",
    "# Save if needed\n",
    "image.save(\"synthetic_damage_sample.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7ea54-582c-41d6-8548-5df1eb7d5e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4186b7a-1605-49d4-93d7-5fd08ce0c612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a7cca7b-aa72-45c1-acf2-82495ddfc5fb",
   "metadata": {},
   "source": [
    "# Trying to train the model on all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14accaa0-3106-4543-9a2b-71067d083426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\diffusers\\pipelines\\pipeline_loading_utils.py:285: FutureWarning: You are loading the variant fp16 from CompVis/stable-diffusion-v1-4 via `revision='fp16'`. This behavior is deprecated and will be removed in diffusers v1. One should use `variant='fp16'` instead. However, it appears that CompVis/stable-diffusion-v1-4 currently does not have the required variant filenames in the 'main' branch. \n",
      " The Diffusers team and community would be very grateful if you could open an issue: https://github.com/huggingface/diffusers/issues/new with the title 'CompVis/stable-diffusion-v1-4 is missing fp16 files' so that the correct variant file can be added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be91e9eaf3e45febd9191c3c3bbdb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "D:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "An error occurred while trying to fetch C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    revision=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Now, processor is:\n",
    "processor = pipe\n",
    "\n",
    "processor = {\n",
    "    \"image_processor\": pipe.feature_extractor,  # For images\n",
    "    \"tokenizer\": pipe.tokenizer,                # For captions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "673c7449-97bc-4905-983f-b2a31a43b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectImageCaptionDataset(Dataset):\n",
    "    def __init__(self, data_folder, processor, size=(512, 512)):\n",
    "        self.data_folder = data_folder\n",
    "        self.processor = processor\n",
    "        self.size = size\n",
    "\n",
    "        # List all image files\n",
    "        self.image_files = [f for f in os.listdir(data_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Image filename\n",
    "        image_filename = self.image_files[idx]\n",
    "        image_path = os.path.join(self.data_folder, image_filename)\n",
    "\n",
    "        # Corresponding caption filename\n",
    "        caption_filename = os.path.splitext(image_filename)[0] + '.txt'\n",
    "        caption_path = os.path.join(self.data_folder, caption_filename)\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = image.resize(self.size)\n",
    "\n",
    "        # Load caption\n",
    "        with open(caption_path, 'r', encoding='utf-8') as f:\n",
    "            caption = f.read().strip()\n",
    "\n",
    "        # Processor\n",
    "        inputs = self.processor(images=image, text=caption, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62820964-d94c-411e-bb78-60ed48dd1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DefectImageCaptionDataset(\n",
    "    data_folder=\"../dataset/bottle/image/\",\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0aca36d6-8bb3-4eaa-bff3-83426c53a939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      7\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader)\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m      9\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     11\u001b[0m         latents \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mencode(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mhalf())\u001b[38;5;241m.\u001b[39mlatent_dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mD:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mD:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[22], line 31\u001b[0m, in \u001b[0;36mDefectImageCaptionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     28\u001b[0m     caption \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Processor\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaption\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     36\u001b[0m }\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, unet.parameters()), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "unet.train()\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        latents = pipe.vae.encode(batch[\"pixel_values\"].to(\"cuda\").half()).latent_dist.sample()\n",
    "        latents = latents * 0.18215  # VAE scaling\n",
    "\n",
    "        encoder_hidden_states = pipe.text_encoder(batch[\"input_ids\"].to(\"cuda\"))[0]\n",
    "\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=latents.device).long()\n",
    "\n",
    "        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3871b0-1a41-4d31-a9fc-7a1f5ae13e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
