{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a513f24a-7722-4717-8130-2733410612fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL, DDPMScheduler\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ec166c-b5ac-4e13-8fa0-128c6f1ff57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb42520-9605-45c1-ad6a-92d13c3eb3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\diffusers\\pipelines\\pipeline_loading_utils.py:285: FutureWarning: You are loading the variant fp16 from CompVis/stable-diffusion-v1-4 via `revision='fp16'`. This behavior is deprecated and will be removed in diffusers v1. One should use `variant='fp16'` instead. However, it appears that CompVis/stable-diffusion-v1-4 currently does not have the required variant filenames in the 'main' branch. \n",
      " The Diffusers team and community would be very grateful if you could open an issue: https://github.com/huggingface/diffusers/issues/new with the title 'CompVis/stable-diffusion-v1-4 is missing fp16 files' so that the correct variant file can be added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9fc60605bd43d0b903cdcb988b3ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\adity\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4\\snapshots\\2880f2ca379f41b0226444936bb7a6766a227587\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "D:\\Python\\siemens-rag-bot\\env\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    revision=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe = pipe.to(device)\n",
    "pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc46af8-f14c-4909-bf8a-b50283c0aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = pipe.unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445a43a2-1347-4f57-a8ac-51ea1eca650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA injected successfully into UNet!\n"
     ]
    }
   ],
   "source": [
    "# Set LoRA Adapters for UNet\n",
    "rank = 4  # LoRA rank\n",
    "\n",
    "# Iterate through all attention processors\n",
    "for name, module in unet.attn_processors.items():\n",
    "    if isinstance(module, LoRAAttnProcessor):\n",
    "        continue  # Already LoRA\n",
    "    cross_attention_dim = module.cross_attention_dim if hasattr(module, \"cross_attention_dim\") else None\n",
    "    hidden_size = module.hidden_size if hasattr(module, \"hidden_size\") else None\n",
    "\n",
    "    if cross_attention_dim is None or hidden_size is None:\n",
    "        continue\n",
    "\n",
    "    # Create LoRA processor\n",
    "    lora_attn_processor = LoRAAttnProcessor(\n",
    "        hidden_size=hidden_size,\n",
    "        cross_attention_dim=cross_attention_dim,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "    # Set it\n",
    "    unet.set_attn_processor(name, lora_attn_processor)\n",
    "\n",
    "print(\"LoRA injected successfully into UNet!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c162250-2f5c-4bfe-86c4-113114fa10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectImageCaptionDataset(Dataset):\n",
    "    def __init__(self, data_folder, image_size=512):\n",
    "        self.data_folder = data_folder\n",
    "        self.image_paths = []\n",
    "        self.caption_paths = []\n",
    "        for file in os.listdir(data_folder):\n",
    "            if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
    "                img_path = os.path.join(data_folder, file)\n",
    "                txt_path = img_path.replace(\".png\", \".txt\").replace(\".jpg\", \".txt\")\n",
    "                if os.path.exists(txt_path):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.caption_paths.append(txt_path)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        with open(self.caption_paths[idx], \"r\") as f:\n",
    "            caption = f.read().strip()\n",
    "        \n",
    "        return {\"image\": img, \"caption\": caption}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc55924-e1e4-425c-977b-65fd4be4c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DefectImageCaptionDataset(data_folder=\"../dataset/bottle/image/\")\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a80460-df91-4ad7-82e6-29314c5fe3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=0.0001)\n",
    "num_epochs = 5\n",
    "processor = pipe\n",
    "# processor = {\n",
    "#     \"image_processor\": pipe.feature_extractor,  # For images\n",
    "#     \"tokenizer\": pipe.tokenizer,                # For captions\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60381640-d5a0-4a09-921a-dfd52993ac64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__ior__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__match_args__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__post_init__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'images',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'move_to_end',\n",
       " 'nsfw_content_detected',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'to_tuple',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(text_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a304e6e1-e4b3-4eed-8254-14ea134b52b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_19424\\1956432704.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8792938af9a7445c9933c45df1345121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/63 [00:08<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StableDiffusionPipelineOutput(images=[<PIL.Image.Image image mode=RGB size=512x512 at 0x167036B1360>], nsfw_content_detected=[False])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StableDiffusionPipelineOutput' object has no attribute 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 35\u001b[0m\n\u001b[0;32m     27\u001b[0m text_inputs \u001b[38;5;241m=\u001b[39m processor(\n\u001b[0;32m     28\u001b[0m     captions,\n\u001b[0;32m     29\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_inputs)\n\u001b[1;32m---> 35\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtext_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Encode text\u001b[39;00m\n\u001b[0;32m     38\u001b[0m encoder_hidden_states \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mtext_encoder(input_ids)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StableDiffusionPipelineOutput' object has no attribute 'input_ids'"
     ]
    }
   ],
   "source": [
    "unet.train()\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-4\n",
    "gradient_accumulation_steps = 1  # can be >1 if you want\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Optimizer and Scaler\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Training Loop\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    unet.train()\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        with torch.no_grad():\n",
    "            pixel_values = (batch[\"image\"].to(device) / 127.5) - 1.0  # scale images to [-1, 1]\n",
    "\n",
    "            captions = batch[\"caption\"]  # list of caption strings\n",
    "\n",
    "            # Tokenize captions on the fly\n",
    "            text_inputs = processor(\n",
    "                captions,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            print(text_inputs)\n",
    "            input_ids = text_inputs.input_ids.to(device)\n",
    "            \n",
    "            # Encode text\n",
    "            encoder_hidden_states = pipe.text_encoder(input_ids)[0]\n",
    "\n",
    "            # Sample random noise\n",
    "            noise = torch.randn_like(pixel_values)\n",
    "            timesteps = torch.randint(\n",
    "                0, pipe.scheduler.config.num_train_timesteps,\n",
    "                (pixel_values.shape[0],),\n",
    "                device=device\n",
    "            ).long()\n",
    "\n",
    "            # Add noise to images\n",
    "            noisy_images = pipe.scheduler.add_noise(pixel_values, noise, timesteps)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Predict the noise residual\n",
    "            model_pred = unet(noisy_images, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "            # Loss\n",
    "            loss = torch.nn.functional.mse_loss(model_pred, noise)\n",
    "\n",
    "        # Backward\n",
    "        scaler.scale(loss / gradient_accumulation_steps).backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(unet.parameters(), max_grad_norm)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Print loss\n",
    "        if step % 10 == 0:\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch} completed.\")\n",
    "\n",
    "print(\"Training Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a119e-6d05-4a3e-9a48-3b94d1ba9197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba833cd-7e4a-4081-af38-6531094a9849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
